---
title: "XGBoost"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Preparation

```{r}
# Load data
train_data <- read.csv('data_train_lasso.csv')
test_data <- read.csv('data_test_lasso.csv')
```


## Model Fitting

```{r}
# Load required libraries
library(xgboost)
library(caret)
library(pROC)
library(plotROC)
library(ggplot2)
library(rms)
```


```{r}
# Prepare matrices for XGBoost
X_train <- train_data[, -1]
y_train <- train_data$disease
y_train_factor <- factor(y_train, levels = c(0, 1))

X_test <- test_data[, -1]
y_test <- test_data$disease
y_test_factor <- factor(y_test, levels = c(0, 1))

# convert to a matrices
X_train_matrix <- as.matrix(X_train)
X_test_matrix <- as.matrix(X_test)

```


```{r}
# Define the parameter grid correctly
xgb_param_grid <- expand.grid(
  nrounds = c(100, 200, 300),
  max_depth = c(3, 4, 5),
  eta = c(0.1, 0.01, 0.001),
  gamma = 0,  # Add other parameters as necessary
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

# Train the model using caret
xgb_model <- train(
  x = X_train,
  y = y_train,
  method = "xgbTree",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = xgb_param_grid
)

# Get the best model
best_xgb_model <- xgb_model$finalModel

```


```{r}
# get predictions
predictions <- predict(best_xgb_model, newdata = X_test_matrix)

# convert probabilities in `predictions` to class labels
threshold = 0.5
class_predictions = ifelse(predictions > threshold, 1, 0)
class_predictions_factor <- factor(class_predictions)

```


## Metrics
```{r} 
# Compute ROC curve
roc_curve <- roc(y_test, predictions)
# Calculate the AUC
auc_value <- auc(roc_curve)
print(auc_value)

#######################

# Create a confusion matrix
conf_matrix <- confusionMatrix(class_predictions_factor,
                               y_test_factor)

# Print the confusion matrix
print(conf_matrix)
```


```{r}
# Prepare the data for ggplot
df_for_plot <- data.frame(D = y_test, M = predictions)

# Calculate the AUC
roc_curve <- roc(df_for_plot$D, df_for_plot$M)
auc_value <- auc(roc_curve)

# Create the ROC plot with AUC
roc_plot <- ggplot(df_for_plot, aes(m = M, d = D)) +
  geom_roc() +
  style_roc() +
  geom_text(aes(x = 0.5, y = .1, 
                label = paste("AUC =", round(auc_value, 5))), 
            size = 5)

# Plot ROC with AUC
print(roc_plot)
```


```{r}
# Prepare data for Calibration Curve
calibration_data <- data.frame(disease = test_data$disease, predicted = predictions)

# Fit logistic regression model for calibration
calib_model <- lrm(disease ~ predicted, data=calibration_data, x=TRUE, y=TRUE)

# Plot Calibration Curve
calib_plot <- calibrate(calib_model, method="boot", B=1000)
plot(calib_plot, main="Calibration Curve for XGBoost")

```








```{r}
# Get feature importance
importance_matrix <- xgb.importance(
  feature_names = colnames(X_train),
  model = best_xgb_model)

# Print feature importance
print(importance_matrix)

# Plot feature importance
xgb.plot.importance(importance_matrix)


```

Gain, Cover, and Frequency: XGBoost feature importance can be evaluated in terms of gain (the average gain of a feature when it is used in trees), cover (the average coverage of a feature when it is used in trees), and frequency (the relative frequency of a feature being used in trees).


```{r}
# Plotting Gain, Cover, and Frequency
xgb.plot.importance(importance_matrix, measure = "Gain")
xgb.plot.importance(importance_matrix, measure = "Cover")
xgb.plot.importance(importance_matrix, measure = "Frequency")

```



